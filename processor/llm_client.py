# ======================================================
# processor/llm_client.py
# HYBRID Context-Aware LLM Client (Final Compatibility Fix)
# (Responses API + ChatCompletion Fallback + History)
# ======================================================

import os
import logging
from typing import List, Optional, Dict, Any
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI, OpenAIError

# -------------------- CONFIG LOAD --------------------
BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(BASE_DIR / ".env")

MODEL_PRIMARY = os.getenv("OPENAI_MODEL", "gpt-5-mini")
MODEL_FALLBACK = os.getenv("OPENAI_FALLBACK_MODEL", "gpt-4o-mini") 

OPENAI_KEY = os.getenv("OPENAI_API_KEY", "")
DISABLE_FALLBACK = os.getenv("OPENAI_DISABLE_FALLBACK", "0") == "1"
MAX_TOKENS = int(os.getenv("OPENAI_MAX_TOKENS", "4000"))

log = logging.getLogger("llm_client")

# -------------------- INIT CLIENT --------------------
_client = None
if OPENAI_KEY:
    try:
        _client = OpenAI(api_key=OPENAI_KEY)
        log.info(f"‚úÖ OpenAI client initialized (Primary: {MODEL_PRIMARY}, Fallback: {MODEL_FALLBACK})")
    except Exception as e:
        log.error(f"‚ùå Failed to initialize OpenAI client: {e}")
else:
    log.warning("‚ö†Ô∏è OPENAI_API_KEY not found in .env ‚Äî LLM responses may fail.")


# ======================================================
# INTERNAL HELPERS (–®–ò–ù–≠–ß–õ–≠–ì–î–°–≠–ù)
# ======================================================

def _call_responses(system_text: str, user_text: str) -> str:
    """
    (1) “Æ–Ω–¥—Å—ç–Ω API (Stateless - —Ç“Ø“Ø—Ö–≥“Ø–π)
    """
    if not _client:
        raise OpenAIError("OpenAI —Ç“Ø–ª—Ö“Ø“Ø—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–∞–π–Ω–∞.")

    try:
        r = _client.responses.create(
            model=MODEL_PRIMARY,
            input=[
                {"role": "system", "content": [{"type": "input_text", "text": system_text}]},
                {"role": "user", "content": [{"type": "input_text", "text": user_text}]},
            ],
            max_output_tokens=MAX_TOKENS,
        )
        return (r.output_text or "").strip()
    except Exception as e:
        raise OpenAIError(f"Responses API error: {e}")


def _call_chat(system_text: str, user_text: str, history: Optional[List[Dict[str, str]]] = None) -> str:
    """
    (2) –ù”©”©—Ü API (Stateful - —Ç“Ø“Ø—Ö—Ç—ç–π)
    """
    if not _client:
        raise OpenAIError("OpenAI —Ç“Ø–ª—Ö“Ø“Ø—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–∞–π–Ω–∞.")

    # (üü° –¢–ê–ê–†–£–£–õ–ê–• –•–≠–°–≠–ì - –¢–ê–ù–´ –•“Æ–°–≠–õ–¢–ò–ô–ù –î–ê–ì–£–£ –ù–≠–ú–≠–ì–î–õ–≠–≠)
    # processor.py-—Å list[str] –∏—Ä–≤—ç–ª list[dict] –±–æ–ª–≥–æ–∂ —Ö”©—Ä–≤“Ø“Ø–ª–Ω—ç.
    if history and all(isinstance(h, str) for h in history):
        log.warning("History format mismatch (list[str]). Converting to list[dict] (user roles only).")
        # –ó”©–≤—Ö”©–Ω —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç—É—É–¥ —Ç—É–ª –±“Ø–≥–¥–∏–π–≥ "user" role-—Ç–æ–π –±–æ–ª–≥–æ–Ω–æ
        history = [{"role": "user", "content": h} for h in history]
    # (üü° –¢–ê–ê–†–£–£–õ–ê–• –•–≠–°–≠–ì - –¢”®–ì–°”®–í)

    messages = []
    
    # 1. System –∑–∞–∞–≤–∞—Ä
    messages.append({"role": "system", "content": system_text})

    # 2. History (–Ø—Ä–∏–∞–Ω—ã —Ç“Ø“Ø—Ö - –æ–¥–æ–æ –∑”©–≤ dict —Ñ–æ—Ä–º–∞—Ç–∞–∞—Ä –æ—Ä–Ω–æ)
    if history:
        for h in history:
            if isinstance(h, dict) and "role" in h and "content" in h:
                messages.append(h)
    
    # 3. –û–¥–æ–æ–≥–∏–π–Ω –∞—Å—É—É–ª—Ç
    messages.append({"role": "user", "content": user_text})

    try:
        r = _client.chat.completions.create(
            model=MODEL_FALLBACK, 
            messages=messages,
            temperature=0.5,
            max_tokens=MAX_TOKENS,
        )
        return (r.choices[0].message.content or "").strip()
    except Exception as e:
        raise OpenAIError(f"ChatCompletion fallback error: {e}")


# ======================================================
# MAIN WRAPPER
# ======================================================
def ask_llm(prompt: str, history: Optional[List[Dict[str, str]]] = None) -> str:
    """
    Hybrid Context-Aware LLM wrapper:
    - –•—ç—Ä—ç–≤ 'history' –±–∞–π–≤–∞–ª, —à—É—É–¥ stateful _call_chat-–≥ –¥—É—É–¥–Ω–∞.
    - –•—ç—Ä—ç–≤ 'history' –±–∞–π—Ö–≥“Ø–π –±–æ–ª, stateless _call_responses-–≥ –æ—Ä–æ–ª–¥–æ–∂, 
      –∞–ª–¥–∞–∞ –≥–∞—Ä–≤–∞–ª _call_chat —Ä—É—É fallback —Ö–∏–π–Ω—ç.
    """
    if not _client:
        return "‚ö†Ô∏è OpenAI —Ç“Ø–ª—Ö“Ø“Ø—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–∞–π–Ω–∞. .env –¥–∞—Ö—å OPENAI_API_KEY-–≥ —à–∞–ª–≥–∞–Ω–∞ —É—É."

    system_text = (
        "–¢–∞ –±–æ–ª Unitel AI Assistant ‚Äî —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –æ—Ä—É—É–ª—Å–∞–Ω ”©–≥”©–≥–¥”©–ª, —Ñ–∞–π–ª, —è—Ä–∏–∞–Ω—ã —Ç“Ø“Ø—Ö–∏–π–≥ “Ø–Ω–¥—ç—Å–ª—ç–Ω "
        "–ú–æ–Ω–≥–æ–ª –±–æ–ª–æ–Ω –ê–Ω–≥–ª–∏ —Ö—ç–ª—ç—ç—Ä –æ–π–ª–≥–æ–º–∂—Ç–æ–π, Markdown —Ñ–æ—Ä–º–∞—Ç—Ç–∞–π–≥–∞–∞—Ä —Ö–∞—Ä–∏—É–ª—Ç ”©–≥–¥”©–≥ –º—ç—Ä–≥—ç–∂–ª–∏–π–Ω —Ç—É—Å–ª–∞—Ö —é–º."
    )

    try:
        # --- (A) CONTEXT-AWARE (–¢“Ø“Ø—Ö—Ç—ç–π) ---
        if history:
            log.info("Context detected. Using stateful chat completion API (_call_chat).")
            # _call_chat –Ω—å –¥–æ—Ç—Ä–æ–æ history-–≥ —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö –ª–æ–≥–∏–∫—Ç–æ–π –±–æ–ª—Å–æ–Ω
            return _call_chat(system_text, prompt, history=history)

        # --- (B) STATELESS (–¢“Ø“Ø—Ö–≥“Ø–π, —à–∏–Ω—ç –∞—Å—É—É–ª—Ç) ---
        log.info("No context. Using stateless hybrid logic (_call_responses -> _call_chat).")
        try:
            # 1. “Æ–Ω–¥—Å—ç–Ω API-–≥ –æ—Ä–æ–ª–¥–æ—Ö (Stateless)
            return _call_responses(system_text, prompt)
        
        except OpenAIError as e:
            msg = str(e)
            log.warning(f"‚ö†Ô∏è LLM primary error: {msg}")

            bad_request = any(x in msg for x in ["400", "invalid_request_error", "Unsupported", "Responses API error"])
            
            if not DISABLE_FALLBACK and bad_request:
                log.info("Fallback activated. Calling _call_chat (stateless).")
                # –ù”©”©—Ü API-–≥ –¥—É—É–¥–∞—Ö (history-–≥“Ø–π–≥—ç—ç—Ä)
                return _call_chat(system_text, prompt, history=None)
            
            raise e

    # --- (C) –ï–†”®–ù–•–ò–ô –ê–õ–î–ê–ê–ù–´ –£–î–ò–†–î–õ–ê–ì–ê ---
    except OpenAIError as e:
        msg = str(e)
        if "insufficient_quota" in msg:
            return "‚ö†Ô∏è OpenAI –∫–≤–æ—Ç –¥—É—É—Å—Å–∞–Ω –±–∞–π–Ω–∞."
        if "rate_limit" in msg:
            return "‚ö†Ô∏è –•—ç—Ç –æ–ª–æ–Ω —Ö“Ø—Å—ç–ª—Ç –∏–ª–≥—ç—ç–≥–¥—ç–∂ –±–∞–π–Ω–∞. –¢“Ø—Ä —Ö“Ø–ª—ç—ç–≥—ç—ç–¥ –¥–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É."
        log.error(f"‚ùå LLM Error: {msg}")
        return f"‚ö†Ô∏è LLM –∞–ª–¥–∞–∞: {msg}"

    except Exception as e:
        log.exception("‚ùå Unexpected LLM system exception: %s", e)
        return f"‚ö†Ô∏è LLM —Å–∏—Å—Ç–µ–º–∏–π–Ω –∞–ª–¥–∞–∞: {e}"


# ======================================================
# QUICK LOCAL TEST
# ======================================================
if __name__ == "__main__":
    print("üîç LLM Hybrid Context-Aware Test:")
    
    # Test 1: Stateless (No History)
    print("\n--- Test 1: Stateless (No History) ---")
    print(f"ASSISTANT: {ask_llm('–°–∞–π–Ω —É—É? –≠–Ω—ç —Å–∏—Å—Ç–µ–º –∞–∂–∏–ª–ª–∞–∂ –±–∞–π–Ω–∞ —É—É?')}")

    # Test 2: Stateful (With dict History - –∑”©–≤ –∞–∂–∏–ª–ª–∞—Ö —ë—Å—Ç–æ–π)
    print("\n--- Test 2: Stateful (Correct dict History) ---")
    dict_history = [
        {"role": "user", "content": "–ú–∏–Ω–∏–π —Ö–∞–º–≥–∏–π–Ω –¥—É—Ä—Ç–∞–π ”©–Ω–≥”© –±–æ–ª —Ü—ç–Ω—Ö—ç—Ä."},
        {"role": "assistant", "content": "–û–π–ª–≥–æ–ª–æ–æ, —Ç–∞–Ω—ã –¥—É—Ä—Ç–∞–π ”©–Ω–≥”© —Ü—ç–Ω—Ö—ç—Ä —é–º –±–∞–π–Ω–∞."}
    ]
    print(f"ASSISTANT: {ask_llm('–ú–∏–Ω–∏–π –¥—É—Ä—Ç–∞–π ”©–Ω–≥”© —é—É –≤—ç?', history=dict_history)}")

    # Test 3: Stateful (With str History - —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö —ë—Å—Ç–æ–π)
    print("\n--- Test 3: Stateful (Incorrect str History - Auto-converting) ---")
    str_history = [
        "–ú–∏–Ω–∏–π –Ω—ç—Ä–∏–π–≥ –ë–æ–ª–¥ –≥—ç–¥—ç–≥."
    ]
    # –≠–Ω—ç –∞—Å—É—É–ª—Ç–∞–¥ "–ë–æ–ª–¥" –≥—ç–∂ —Ö–∞—Ä–∏—É–ª–∞—Ö —ë—Å—Ç–æ–π
    print(f"ASSISTANT: {ask_llm('–ú–∏–Ω–∏–π –Ω—ç—Ä —Ö—ç–Ω –±—ç?', history=str_history)}")