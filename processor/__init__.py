# ======================================================
# processor/__init__.py ‚Äî Unified Chat Processor (Context-Aware)
# (–•–æ—ë—Ä —Ö—É–≤–∏–ª–±–∞—Ä—ã–≥ –Ω—ç–≥—Ç–≥—ç—Å—ç–Ω)
# ======================================================

from __future__ import annotations
import os, json, random, pickle, re, numpy as np
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any

from keras.models import load_model
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import wordpunct_tokenize

# --- –ù—ç–≥—Ç–≥—ç—Å—ç–Ω Imports ---
from .file_handler import extract_text_from_file, extract_records_from_file
from .prompt_builder import (
    build_prompt, 
    build_general_prompt,
    build_sentiment_prompt, # (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å)
    detect_intent,          # (–•—É—É—á–∏–Ω —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å)
    has_textual_field,      # (–•—É—É—á–∏–Ω —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å)
    safe_json               # (–•—É—É—á–∏–Ω —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å)
)
from .llm_client import ask_llm
# --- ”®”®–†–ß–õ”®–ì–î–°”®–ù IMPORT ---
from .sentiment_analyzer import analyze_sentiment, analyze_sentiment_ai
from .data_connector import fetch_graph_data

# -------- CONTEXT MEMORY (2.1) -----------
# (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å)
USER_CONTEXTS = {}

def get_user_context(session_id: str):
    """–°–µ—à–Ω –±“Ø—Ä–∏–π–Ω —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω context (history, last_file) —Ö–∞–¥–≥–∞–ª–∞—Ö."""
    return USER_CONTEXTS.get(session_id, {"history": [], "last_file": None})

def update_user_context(session_id: str, message: str, file_meta: str = None):
    ctx = USER_CONTEXTS.setdefault(session_id, {"history": [], "last_file": None})
    ctx["history"].append(message) 
    if file_meta:
        ctx["last_file"] = file_meta
    ctx["history"] = ctx["history"][-20:]

# ======================================================
# INITIAL SETUP
# (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å)
# ======================================================
BASE_DIR = Path(__file__).resolve().parent.parent
lemmatizer = WordNetLemmatizer()

try:
    intents = json.loads((BASE_DIR / "job_intents.json").read_text(encoding="utf-8"))
    words = pickle.load(open(BASE_DIR / "words.pkl", "rb"))
    classes = pickle.load(open(BASE_DIR / "classes.pkl", "rb"))
    model = load_model(BASE_DIR / "chatbot_model.h5")
except Exception:
    intents = words = classes = model = None


# ======================================================
# INTENT UTILITIES
# (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å)
# ======================================================
def clean_sentence(s: str) -> list[str]:
    toks = wordpunct_tokenize(s)
    return [lemmatizer.lemmatize(w.lower()) for w in toks]

def bow(sentence: str, vocab: list[str]):
    sw = clean_sentence(sentence)
    return [1 if w in sw else 0 for w in vocab]

def predict_intent(sentence: str, threshold=0.4):
    if not model or not words or not classes:
        return []
    res = model.predict(np.array([bow(sentence, words)]), verbose=0)[0]
    ranked = [[i, r] for i, r in enumerate(res) if r > threshold]
    ranked.sort(key=lambda x: x[1], reverse=True)
    return [{"intent": classes[i], "p": float(p)} for i, p in ranked]

def intent_reply(msg: str) -> Optional[str]:
    ranked = predict_intent(msg, threshold=float(os.getenv("UNITEL_INTENT_THRESHOLD", "0.72")))
    if not ranked:
        return None
    top = ranked[0]["intent"]
    for it in intents.get("intents", []):
        if it.get("tag") == top and it.get("responses"):
            return random.choice(it["responses"])
    return None


# ======================================================
# MAIN CHAT PROCESSOR (–ù–≠–ì–¢–ì–≠–°–≠–ù)
# ======================================================
def process_query(msg: str,
                  session_id: str, # <--- (–®–∏–Ω—ç)
                  files: Optional[List[str]] = None,
                  user: Optional[str] = None) -> str:
    """
    Unified logic for Chatbot: (Context-Aware)
    - (2.2a) session_id –∞—à–∏–≥–ª–∞–Ω context (history, last_file) –∞–≤–Ω–∞
    - (2.2b) –§–∞–π–ª—Ç–∞–π “Ø–µ–¥ context-–≥ —à–∏–Ω—ç—á–∏–ª–Ω—ç (last_file) - (–•—É—É—á–∏–Ω –∫–æ–¥—ã–Ω –Ω–∞—Ä–∏–π–≤—á–∏–ª—Å–∞–Ω –ª–æ–≥–∏–∫ –∞—à–∏–≥–ª–∞–Ω–∞)
    - (2.2c) –§–∞–π–ª–≥“Ø–π “Ø–µ–¥ context-—Å (last_file, history) –∞—à–∏–≥–ª–∞–Ω–∞
    """
    msg = (msg or "").strip()
    if not msg:
        return "‚ö†Ô∏è –•–æ–æ—Å–æ–Ω –∞—Å—É—É–ª—Ç –±–∞–π–Ω–∞."

    # --- (2.2a) Context –∞–≤–∞—Ö (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å) ---
    context = get_user_context(session_id)
    history = context.get("history", [])
    last_file_meta = context.get("last_file")
    # ---------------------------

    files = files or []
    lower = msg.lower()

    # ---------- 1Ô∏è‚É£ FILE ATTACHED (2.2b) ----------
    # (–•—É—É—á–∏–Ω —Ö—É–≤–∏–ª–±–∞—Ä—ã–Ω –Ω–∞—Ä–∏–π–≤—á–∏–ª—Å–∞–Ω —Ñ–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö –ª–æ–≥–∏–∫–∏–π–≥
    # –®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä—ã–Ω context/history-—Ç—ç–π –Ω—ç–≥—Ç–≥—ç–≤)
    if files:
        all_records: list[dict] = []
        blobs: list[tuple[str, str]] = []
        file_meta = "Uploaded File"

        # (–•—É—É—á–∏–Ω –∫–æ–¥—ã–Ω –ª–æ–≥–∏–∫: record/blob —è–ª–≥–∞—Ö)
        for f in files:
            try:
                recs, meta1 = extract_records_from_file(f)
                if recs:
                    all_records.extend(recs)
                    file_meta = meta1
                else:
                    text, meta2 = extract_text_from_file(f)
                    blobs.append((text, meta2))
                    file_meta = meta2
            except Exception as e:
                blobs.append((f"[extract_error:{e}]", f"[{Path(f).name}]"))

        # (–•—É—É—á–∏–Ω –∫–æ–¥—ã–Ω –ª–æ–≥–∏–∫: sentiment —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö)
        wants_sentiment = any(k in lower for k in [
            "sentiment","—Å—ç—Ç–≥—ç–≥–¥—ç–ª","feedback","comment","tone","—ç–µ—Ä—ç–≥","—Å”©—Ä”©–≥"
        ])

        # (–•—É—É—á–∏–Ω –∫–æ–¥—ã–Ω –ª–æ–≥–∏–∫: –•“Ø—Å–Ω—ç–≥—Ç—ç–Ω ”©–≥”©–≥–¥”©–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö)
        if all_records:
            intent_guess = detect_intent(msg, file_meta, all_records)
            textish = has_textual_field(all_records)

            # --- –°–≠–¢–ì–≠–ì–î–≠–õ–¢–≠–ô –§–ê–ô–õ: (AI / –õ–û–ö–ê–õ) ---
            # (–≠–ù–≠ –•–≠–°–≠–ì –¢–ê–ù–´ –•“Æ–°–≠–õ–¢–ò–ô–ù –î–ê–ì–£–£ ”®”®–†–ß–õ”®–ì–î–õ”®”®)
            if wants_sentiment and textish:
                # === AI –±—É—é—É LLM-–¥ —Å—É—É—Ä–∏–ª—Å–∞–Ω —Ö—É–≤–∏–ª–±–∞—Ä ===
                USE_AI_SENTIMENT = True   # <== toggle: True –±–æ–ª AI –∞—à–∏–≥–ª–∞–Ω–∞, False –±–æ–ª —Ö—É—É—á–∏–Ω Lexicon

                if USE_AI_SENTIMENT:
                    answer = analyze_sentiment_ai(all_records, meta=file_meta)
                    update_user_context(session_id, msg, file_meta=file_meta)
                    return answer
                else:
                    s = analyze_sentiment(all_records)
                    payload = {
                        "meta": file_meta,
                        "counts": s["counts"],
                        "ratios": s["ratios"],
                        "examples": s["examples"],
                    }
                    prompt = (
                        "–î–æ–æ—Ä—Ö –Ω—å —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –°–≠–¢–ì–≠–ì–î–õ–ò–ô–ù –±–æ–¥–∏—Ç —Ç–æ–æ—Ü–æ–æ (–ª–æ–∫–∞–ª) —é–º. "
                        "–¢–æ–æ–Ω—É—É–¥—ã–≥ ”©”©—Ä—á–ª”©—Ö–≥“Ø–π. Markdown —Ç–∞–π–ª–∞–Ω –±–∏—á–∏–∂, —Ö—É–≤—å, –¥“Ø–≥–Ω—ç–ª—Ç, 3‚Äì5 –≥–æ–ª —Å—ç–¥—ç–≤, "
                        "—Ç”©–ª”©”©–ª”©—Ö –∏—à–ª—ç–ª“Ø“Ø–¥–∏–π–≥ –æ—Ä—É—É–ª. –ú–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä –±–∏—á.\n\n"
                        + safe_json(payload)
                    )
                    answer = ask_llm(prompt, history=history)
                    update_user_context(session_id, msg, file_meta=file_meta)
                    return answer

            # --- STRUCTURED –±—É—é—É —Ç–æ–æ–Ω ”©–≥”©–≥–¥”©–ª ---
            if intent_guess in ("influencer", "ad_report", "numeric", "general"):
                prompt = build_prompt(msg, all_records, meta=file_meta)
                # (–ù—ç–≥—Ç–≥—ç—Å—ç–Ω: history –±–æ–ª–æ–Ω session_id –∞—à–∏–≥–ª–∞—Ö)
                answer = ask_llm(prompt, history=history)
                update_user_context(session_id, msg, file_meta=file_meta)
                return answer

        # --- –¢–∞–±–ª–∏—á –±–∏—à / —Ç–µ–∫—Å—Ç—ç–Ω —Ñ–∞–π–ª ---
        # (–•—É—É—á–∏–Ω –∫–æ–¥—ã–Ω –ª–æ–≥–∏–∫: Blob –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö)
        if blobs:
            prompt = build_prompt(msg, [ {"blob": t, "meta": m} for (t,m) in blobs ], meta=file_meta)
            # (–ù—ç–≥—Ç–≥—ç—Å—ç–Ω: history –±–æ–ª–æ–Ω session_id –∞—à–∏–≥–ª–∞—Ö)
            answer = ask_llm(prompt, history=history)
            update_user_context(session_id, msg, file_meta=file_meta)
            return answer

        # --- Fallback (–§–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∂ —á–∞–¥–∞–∞–≥“Ø–π) ---
        prompt = build_general_prompt(msg, f"[{file_meta}] structured/–±—É—Å –º–∞—Ç–µ—Ä–∏–∞–ª —Ç–∞–Ω–∏–≥–¥—Å–∞–Ω–≥“Ø–π.")
        # (–ù—ç–≥—Ç–≥—ç—Å—ç–Ω: history –±–æ–ª–æ–Ω session_id –∞—à–∏–≥–ª–∞—Ö)
        answer = ask_llm(prompt, history=history)
        update_user_context(session_id, msg, file_meta=file_meta)
        return answer

    # ---------- 2Ô∏è‚É£ FACEBOOK / INSIGHT QUERIES ----------
    # (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å, context –∞—à–∏–≥–ª–∞—Ö–≥“Ø–π —Ö—ç—Å—ç–≥)
    fb_keywords = ["facebook", "insight", "impression", "reach", "spend", "campaign", "ad", "graph"]
    comment_keywords = ["facebook comment", "fb comment", "–∫–æ–º–º–µ–Ω—Ç —Ç–∞—Ç", "comment —Ç–∞—Ç", "–∫–æ–º–º–µ–Ω—Ç ”©–≥"]

    if any(k in lower for k in fb_keywords + comment_keywords):
        try:
            result = fetch_graph_data(msg)
            if not result:
                return "‚ö†Ô∏è Graph API-–∞–∞—Å ”©–≥”©–≥–¥”©–ª –±—É—Ü–∞–∞–≥–¥—Å–∞–Ω–≥“Ø–π."

            links = []
            if result.get("xlsx_url"):
                links.append(f'üìò <a href="{result["xlsx_url"]}" download target="_blank">XLSX —Ç–∞—Ç–∞—Ö</a>')
            if result.get("json_url"):
                links.append(f'üßæ <a href="{result["json_url"]}" download target="_blank">JSON —Ç–∞—Ç–∞—Ö</a>')
            links_html = "<br>".join(links)

            if "comment" in lower:
                resp = f"‚úÖ {result.get('count', 0)} –∫–æ–º–º–µ–Ω—Ç —Ç–∞—Ç–ª–∞–∞.<br>{links_html}"
            elif "ad" in lower or "insight" in lower:
                resp = f"üìä Ads —Ç–∞–π–ª–∞–Ω –≥–∞—Ä–≥–∞–ª–∞–∞ ({result.get('count', 0)} –º”©—Ä).<br>{links_html}"
            else:
                resp = f"‚úÖ {result.get('count', 0)} –±–∏—á–ª—ç–≥ —Ç–∞—Ç–ª–∞–∞.<br>{links_html}"
            
            # –≠–Ω—ç –Ω—å LLM –±–∏—à —Ç—É–ª context-–¥ —Ö–∞–¥–≥–∞–ª–∞—Ö–≥“Ø–π (—ç—Å–≤—ç–ª –∞—Å—É—É–ª—Ç—ã–≥ —Ö–∞–¥–≥–∞–ª–∂ –±–æ–ª–Ω–æ)
            # update_user_context(session_id, msg) 
            return resp

        except Exception as e:
            return f"‚ö†Ô∏è Facebook Graph API –¥–∞—Ç–∞ —Ç–∞—Ç–∞—Ö–∞–¥ –∞–ª–¥–∞–∞: {e}"

    # ---------- 3Ô∏è‚É£ SENTIMENT DIRECT ----------
    # (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å, context-–¥ —Ö–∞–¥–≥–∞–ª–Ω–∞)
    if re.search(r"(sentiment|—Å—ç—Ç–≥—ç–≥–¥—ç–ª|positive|negative|—ç–µ—Ä—ç–≥|—Å”©—Ä”©–≥)", msg, re.I):
        ans = analyze_sentiment(msg)
        update_user_context(session_id, msg, file_meta=last_file_meta)
        return ans

    # ---------- 4Ô∏è‚É£ LOCAL INTENT ----------
    # (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å, context-–¥ —Ö–∞–¥–≥–∞–ª–Ω–∞)
    ans = intent_reply(msg)
    if ans:
        update_user_context(session_id, msg, file_meta=last_file_meta)
        return ans

    # ---------- 5Ô∏è‚É£ FALLBACK ‚Üí LLM (2.2c) ----------
    # (–®–∏–Ω—ç —Ö—É–≤–∏–ª–±–∞—Ä–∞–∞—Å - context –∞—à–∏–≥–ª–∞–Ω —Ö–∞—Ä–∏—É–ª–∞—Ö)
    
    # (2.2c) –§–∞–π–ª–≥“Ø–π “Ø–µ–¥ last_file_meta-–≥ –∞—à–∏–≥–ª–∞—Ö
    blobs = []
    if last_file_meta:
        blobs.append((f"[–¢–∞–π–ª–±–∞—Ä: –•—ç—Ä—ç–≥–ª—ç–≥—á ”©–º–Ω”© –Ω—å –∞—à–∏–≥–ª–∞—Å–∞–Ω '{last_file_meta}' —Ñ–∞–π–ª—ã–Ω —Ç–∞–ª–∞–∞—Ä –∞—Å—É—É–∂ –±–∞–π–Ω–∞]", last_file_meta))

    base_prompt = build_general_prompt(msg, blobs)

    # (2.2c) History-–≥ –∞—à–∏–≥–ª–∞—Ö
    if history:
        ctx = "\n".join(history[-8:]) 
        final_prompt = f"”®–º–Ω”©—Ö —è—Ä–∏–∞–Ω—ã —Ç–æ–≤—á —Ç“Ø“Ø—Ö:\n{ctx}\n\n–®–∏–Ω—ç –∞—Å—É—É–ª—Ç:\n{base_prompt}"
    else:
        final_prompt = base_prompt

    answer = ask_llm(final_prompt) # History-–≥ prompt-–¥ –æ—Ä—É—É–ª—Å–∞–Ω —Ç—É–ª —ç–Ω–¥ –¥–∞–º–∂—É—É–ª–∞—Ö–≥“Ø–π

    # --- (2.2c) Context —à–∏–Ω—ç—á–ª—ç—Ö ---
    update_user_context(session_id, msg, file_meta=last_file_meta) 
    # (–•–∞—Ä–∏—É–ª—Ç—ã–≥ –º”©–Ω history-–¥ –Ω—ç–º–±—ç–ª –¥–∞—Ä–∞–∞–≥–∏–π–Ω context-–¥ –∞—à–∏–≥–ª–∞–≥–¥–∞–Ω–∞)
    # update_user_context(session_id, answer, file_meta=last_file_meta) 
    
    return answer